# 如何解决训练样本少的问题
> 目前大部分的深度学习模型仍然需要海量的数据支持。例如 ImageNet 数据就拥有 1400 多万的图片。而现实生产环境中，数据集通常较小，只有几万甚至几百个样本。这时候，如何在这种情况下应用深度学习呢？
1.利用预训练模型进行迁移微调（fine-tuning），预训练模型通常在特征上拥有很好的语义表达。此时，只需将模型在小数据集上进行微调就能取得不错的效果。这也是目前大部分小数据集常用的训练方式。视觉领域内，通常会 ImageNet 上训练完成的模型。自然语言处理领域，也有 BERT 模型等预训练模型可以使用。
2.单样本或者少样本学习（one-shot，few-shot learning），这种方式适用于样本类别远远大于样本数量的情况等极端数据集。例如有 1000 个类别，每个类别只提供 1-5 个样本。少样本学习同样也需要借助预训练模型，但有别于微调的在于，微调通常仍然在学习不同类别的语义，而少样本学习通常需要学习样本之间的距离度量。例如孪生网络（Siamese Neural Networks）就是通过训练两个同种结构的网络来判别输入的两张图片是否属于同一类。上述两种是常用训练小样本数据集的方式。此外，也有些常用的手段，例如数据集增强、正则或者半监督学习等方式来解决小样本数据集的训练问题。
# 神经网络中权值共享的理解
> 权值 (权重) 共享这个词是由 LeNet5 模型提出来的。以 CNN 为例，在对一张图偏进行卷积的过程中，使用的是同一个卷积核的参数。比如一个 3×3×1 的卷积核，这个卷积核内 9 个的参数被整张图共享，而不会因为图像内位置的不同而改变卷积核内的权系数。说的再直白一些，就是用一个卷积核不改变其内权系数的情况下卷积处理整张图片（当然 CNN 中每一层不会只有一个卷积核的，这样说只是为了方便解释而已）
# 深度学习是否能胜任所有数据集
> 深度学习并不能胜任目前所有的数据环境，以下列举两种情况：
1.深度学习能取得目前的成果，很大一部分原因依赖于海量的数据集以及高性能密集计算硬件。因此，当数据集过小时，需要考虑与传统机器学习相比，是否在性能和硬件资源效率更具有优势。 
2.深度学习目前在视觉，自然语言处理等领域都有取得不错的成果。这些领域最大的特点就是具有局部相关性。例如图像中，人的耳朵位于两侧，鼻子位于两眼之间，文本中单词组成句子。这些都是具有局部相关性的，一旦被打乱则会破坏语义或者有不同的语义。所以当数据不具备这种相关性的时候，深度学习就很难取得效果。
# 对 fine-tuning (微调模型的理解)，为什么要修改最后几层神经网络权值
> 使用预训练模型的好处，在于利用训练好的 SOTA 模型权重去做特征提取，可以节省我们训练模型和调参的时间。
至于为什么只微调最后几层神经网络权重，是因为：
1.CNN 中更靠近底部的层（定义模型时先添加到模型中的层）编码的是更加通用的可复用特征，而更靠近顶部的层（最后添加到模型中的层）编码的是更专业业化的特征。微调这些更专业化的特征更加有用，它更代表了新数据集上的有用特征。 
2.训练的参数越多，过拟合的风险越大。很多 SOTA 模型拥有超过千万的参数，在一个不大的数据集上训练这么多参数是有过拟合风险的，除非你的数据集像 Imagenet 那样大。
# 有没有可能找到比已知算法更好的算法
> 使用激活函数的目的是为了向网络中加入非线性因素；加强网络的表示能力，解决线性模型无法解决的问题
# 那为什么要使用非线性激活函数？
> 为什么加入非线性因素能够加强网络的表示能力？—— 神经网络的万能近似定理
神经网络的万能近似定理认为主要神经网络具有至少一个非线性隐藏层，那么只要给予网络足够数量的隐藏单元，它就可以以任意的精度来近似任何从一个有限维空间到另一个有限维空间的函数。

如果不使用非线性激活函数，那么每一层输出都是上层输入的线性组合；此时无论网络有多少层，其整体也将是线性的，这会导致失去万能近似的性质

但仅部分层是纯线性是可以接受的，这有助于减少网络中的参数。
# 是否还有其他激活函数？
> 很多未发布的非线性激活函数也能表现的很好，但没有比流行的激活函数表现的更换。比如 cos 也能在 MNIST 任务上得到小于 1% 的误差。通常新的隐藏单元类型只有在被明确证明能够提供显著改进时才会被发布。
# 为什么 ReLU 不是全程可微 / 可导也能用于基于梯度的学习？
> 虽然从数学的角度看 ReLU 在 0 点不可导，因为它的左导数和右导数不相等；

但是在实现时通常会返回左导数或右导数的其中一个，而不是报告一个导数不存在的错误。从而避免了这个问题
# 什么是共线性，如何判断和解决共线性问题？
> 对于回归算法，无论是一般回归还是逻辑回归，在使用多个变量进行预测分析时，都可能存在多变量相关的情况，这就是多重共线性。共线性的存在，使得特征之间存在冗余，导致过拟合。
常用判断是否存在共线性的方法有：
1.相关性分析。当相关性系数高于 0.8，表明存在多重共线性；但相关系数低，并不能表示不存在多重共线性；
2.方差膨胀因子 VIF。当 VIF 大于 5 或 10 时，代表模型存在严重的共线性问题；
3.条件系数检验。当条件数大于 100、1000 时，代表模型存在严重的共线性问题。
通常可通过 PCA 降维、逐步回归法和 LASSO 回归等方法消除共线性。